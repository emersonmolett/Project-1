{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependancies\n",
    "# possibly more than needed\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from scipy.stats import linregress\n",
    "import chardet\n",
    "mildProfanity = \"Rats.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file at Resources/co-est2019-alldata.csv is 3.48 megabytes (MB).\n",
      "It came from:\n",
      "https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv\n",
      "More info here:\n",
      "https://www.census.gov/data/tables/time-series/demo/popest/2010s-counties-total.html\n"
     ]
    }
   ],
   "source": [
    "# 1. US Census 2010-2019\n",
    "# This stores the URL the file came from, in case you want to get it yourself directly from where I did...:\n",
    "censusDataOrigURL = \"https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv\" \n",
    "\n",
    "# ...and this stores the URL of information about the file, as well as methodology:\n",
    "censusDataReadMeURL = \"https://www.census.gov/data/tables/time-series/demo/popest/2010s-counties-total.html\"\n",
    "\n",
    "# This sets the filepath where the census data .CSV lives locally...:\n",
    "censusDataFilepath = \"Resources/co-est2019-alldata.csv\"\n",
    "\n",
    "# ...and this tests that the filepath works,\n",
    "# in that it finds a file of a certain size in megabytes (MB):\n",
    "print(f\"The file at {censusDataFilepath} is {round(os.path.getsize(censusDataFilepath)/1024/1024, 2)} megabytes (MB).\\nIt came from:\\n{censusDataOrigURL}\\nMore info here:\\n{censusDataReadMeURL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gives a spooky error I asked Dom about that looks like it has to do with encoding:\n",
    "# \"UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 2: invalid continuation byte\"\n",
    "censusData = pd.read_csv(censusDataFilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And this errors out: \"NameError: name 'censusData' is not defined\"\n",
    "# because Python couldn't read the CSV file into the variable censusData\n",
    "print(censusData).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a savvy-looking piece of code from\n",
    "# https://krinkere.github.io/krinkersite/encoding_csv_file_python.html\n",
    "# that looks at the first ten thousand bytes of the file to guess the character encoding\n",
    "\n",
    "with open(censusDataFilepath, 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))\n",
    "print(result)\n",
    "\n",
    "# for more on encodings, see:\n",
    "# https://docs.python.org/3/library/codecs.html#standard-encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Krinkere's chardet suggests 100% confidence this file is encoded as ASCII text,\n",
    "# so let's try specifying ASCII:\n",
    "censusData = pd.read_csv(censusDataFilepath, encoding=\"ascii\")\n",
    "# print(censusData).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But that errors out too:\n",
    "# \"UnicodeDecodeError: 'ascii' codec can't decode byte 0xf1 in position 253967: ordinal not in range(128)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rats.\n"
     ]
    }
   ],
   "source": [
    "print(mildProfanity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file at Resources/us-counties.csv is 0.09 MB.\n",
      "It came from:\n",
      "https://github.com/nytimes/covid-19-data/blob/master/live/us-counties.csv\n",
      "More info here:\n",
      "https://github.com/nytimes/covid-19-data/blob/master/README.md\n"
     ]
    }
   ],
   "source": [
    "# But let's keep trying:\n",
    "# 2. COVID-19 cases\n",
    "# This stores the URL the file came from, in case you want to get it yourself directly from where I did...:\n",
    "caseDataOrigURL = \"https://github.com/nytimes/covid-19-data/blob/master/live/us-counties.csv\"\n",
    "\n",
    "# ...and this stores the URL of information about the file, as well as methodology:\n",
    "caseDataReadMeURL = \"https://github.com/nytimes/covid-19-data/blob/master/README.md\"\n",
    "\n",
    "# This sets the filepath where the census data .CSV lives locally...:\n",
    "caseDataFilepath = \"Resources/us-counties.csv\"\n",
    "\n",
    "# ...and this tests that the filepath works,\n",
    "# in that it finds a file of a certain size in megabytes (MB):\n",
    "print(f\"The file at {caseDataFilepath} is {round(os.path.getsize(caseDataFilepath)/1024/1024, 2)} MB.\\nIt came from:\\n{caseDataOrigURL}\\nMore info here:\\n{caseDataReadMeURL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'utf-8', 'confidence': 0.7525, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# How does Krinkere's chardet think this file is encoded?\n",
    "with open(caseDataFilepath, 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))\n",
    "# check what the character encoding might be\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Krinkere's chardet is 75% confident this file is encoded as UTF-8.\n",
    "# So we shouldn't have to specify encoding because UTF-8 is Python's default,\n",
    "# but watch what happens when we try to read the file into a variable:\n",
    "# caseData = pd.read_csv(caseDataFilepath)\n",
    "# A spooky error:\n",
    "# \"ParserError: Error tokenizing data. C error: Expected 1 fields in line 70, saw 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rats.\n"
     ]
    }
   ],
   "source": [
    "print(mildProfanity)\n",
    "# And, as before, printing the head of a variable whose contents couldn't be imported:\n",
    "# print(caseData).head()\n",
    "# returns \"NameError: name 'caseData' is not defined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a 25% chance Krinkere's chardet is wrong about UTF-8, so maybe try different encoding:\n",
    "# caseData = pd.read_csv(caseDataFilepath, encoding = \"ascii\")\n",
    "# This gives a different error:\n",
    "# \"UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 6723: ordinal not in range(128)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe there's something to be gained from telling Python explicitly it's a file encoded as UTF-8:\n",
    "# caseData = pd.read_csv(caseDataFilepath, encoding = \"utf-8\")\n",
    "# But alas, that give the same parser error as when we let Python assume it should use UTF-8 encoding:\n",
    "# \"ParserError: Error tokenizing data. C error: Expected 1 fields in line 70, saw 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file at Resources/mask-use-by-county.csv is 1.08 MB.\n",
      "It came from:\n",
      "https://github.com/nytimes/covid-19-data/blob/bde13b021e99c6b4a63fb66a6144e889cc635e31/mask-use/mask-use-by-county.csv\n",
      "More info here:\n",
      "https://github.com/nytimes/covid-19-data/blob/master/README.md\n"
     ]
    }
   ],
   "source": [
    "# 3. mask-wearing survey\n",
    "\n",
    "# This stores the URL the file came from, in case you want to get it yourself directly from where I did...:\n",
    "maskWearingDataOrigURL = \"https://github.com/nytimes/covid-19-data/blob/bde13b021e99c6b4a63fb66a6144e889cc635e31/mask-use/mask-use-by-county.csv\"\n",
    "\n",
    "# ...and this stores the URL of information about the file, as well as methodology:\n",
    "maskWearingDataReadMeURL = \"https://github.com/nytimes/covid-19-data/blob/master/README.md\"\n",
    "\n",
    "# This sets the filepath where the census data .CSV lives locally...:\n",
    "maskWearingDataFilepath = \"Resources/mask-use-by-county.csv\"\n",
    "\n",
    "# ...and this tests that the filepath works,\n",
    "# in that it finds a file of a certain size in megabytes (MB):\n",
    "print(f\"The file at {maskWearingDataFilepath} is {round(os.path.getsize(maskWearingDataFilepath)/1024/1024, 2)} MB.\\nIt came from:\\n{maskWearingDataOrigURL}\\nMore info here:\\n{maskWearingDataReadMeURL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# Krinkere's chardet\n",
    "with open(maskWearingFilepath, 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))\n",
    "\n",
    "# check what the character encoding might be\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 72, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-d28aa5275548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmaskWearingData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaskWearingFilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaskWearingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/PythonData/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/PythonData/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/PythonData/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/PythonData/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 72, saw 2\n"
     ]
    }
   ],
   "source": [
    "maskWearingData = pd.read_csv(maskWearingFilepath, encoding=\"iso-8859-1\")\n",
    "print(maskWearingData).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
